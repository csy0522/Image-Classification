{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_with_notebook.scripts.util_mnist_reader import load_mnist\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"data_with_notebook/data/fashion\"     # Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ANN import *\n",
    "\n",
    "'''\n",
    "Artifitial Neural Network\n",
    "\n",
    "This is a Neural Network with 1 hidden layer\n",
    "Inputs are 60000 images with 784 pixels\n",
    "Output has 10 categories\n",
    "The prediction is based on the percentage of predictions\n",
    "The highest percentages among all the prediction is the predicted answer.\n",
    "'''\n",
    "### Hyper Parameters used to adjust the result ###\n",
    "hiddens = [[20,],[80,],[300,]]\n",
    "learning_rate = [0.001, 0.0001, 0.00001]\n",
    "activation = 'sigmoid'\n",
    "learning_rate_descent = 1\n",
    "epoch = 10\n",
    "batch_size = 6000\n",
    "\n",
    "### Data ###\n",
    "X,y = get_data(PATH, kind='train', norm_size=255)\n",
    "val_X,val_y = get_validation_data(X,y,0.2)      ### use 20% of the training data for validation ###\n",
    "test_X,test_y = get_data(PATH, kind='t10k', norm_size=255)\n",
    "\n",
    "### Neural Network Model ###\n",
    "ann = ANN(X.shape[1],hiddens[1],10,activation)\n",
    "ann.__initialize_weights_biases__()     \n",
    "ann.__initialize_hyperparameters__(\n",
    "  learning_rate[1],\n",
    "  learning_rate_descent,\n",
    "  epoch,\n",
    "  batch_size)\n",
    "ann.__train__(X,y)\n",
    "\n",
    "plt.plot(np.arange(int(len(X) / batch_size)*epoch),ann.accuracy_)\n",
    "\n",
    "ann.__test__(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLANN import *\n",
    "\n",
    "'''\n",
    "Multilayers Neural Network with Keras\n",
    "Cross Entropy is used for the loss function,\n",
    "Stochasitc Gradient Descent is used for the optimizer\n",
    "Accuracy is used to measure the model accuracy\n",
    "'''     \n",
    "### Hyper Parameters used to adjust the result ###\n",
    "hiddens = [[500,100],[300,50],[500,300,100]]\n",
    "batch_size = [600,5000,10000]\n",
    "\n",
    "activation = 'sigmoid'\n",
    "epoch = 100\n",
    "\n",
    "### Data ###\n",
    "X,y = get_data(PATH, kind='train', norm_size=255)\n",
    "val_X,val_y = get_validation_data(X,y,0.2)      ### use 20% of the training data for validation ###\n",
    "test_X,test_y = get_data(PATH, kind='t10k', norm_size=255)\n",
    "\n",
    "\n",
    "### Neural Network Model with Keras ###\n",
    "mlann = MLANN(X.shape[1],hiddens[0],10,activation)\n",
    "mlann.__build_keras_ann__()\n",
    "mlann.__train__(X,y,(val_X,val_y),epoch,batch_size[1])\n",
    "\n",
    "plt.plot(np.arange(len(mlann.training_info_.history['acc'])),mlann.training_info_.history['acc'])\n",
    "\n",
    "mlann.__test__(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN import *\n",
    "\n",
    "def get_validation_data(X,y,validation_percentage):\n",
    "    validation_X = []\n",
    "    validation_y = []\n",
    "    random_index = np.random.permutation(len(X))\n",
    "    val_size = int(len(X) * validation_percentage)\n",
    "    for i in range(val_size):\n",
    "        validation_X.append(X[random_index[i]])\n",
    "        validation_y.append(y[random_index[i]])\n",
    "    return np.array(validation_X), np.array(validation_y)\n",
    "\n",
    "'''\n",
    "This function initiazes the default layer settings for each type of layers\n",
    "So everytime a new layer is created there will not be repetitive coding\n",
    "'''\n",
    "def get_partial():\n",
    "    def_conv2d = partial(Conv2d_Parameters,filters_=10,kernel_size_=3,strides_=(1,1),padding_='SAME',activation_='relu')\n",
    "    def_pool = partial(Pooling_Parameters,pooling_type_='max', pooling_size_=2)\n",
    "    def_dense = partial(Dense_Parameters,units_=64, activation_='relu')\n",
    "    return def_conv2d, def_pool, def_dense\n",
    "\n",
    "'''\n",
    "Parameters for the CNN model\n",
    "These parameters can be adjusted for a better model\n",
    "'''\n",
    "def get_cnn_params():\n",
    "    def_conv2d, def_pool, def_dense = get_partial()\n",
    "    cnn_input = def_conv2d()\n",
    "    cnn_convs = [def_conv2d()]\n",
    "    cnn_pools = [def_pool()]\n",
    "    cnn_hids = [def_dense()]\n",
    "    cnn_output = def_dense(units_=10,activation_='softmax')\n",
    "    return cnn_input, cnn_convs, cnn_pools, cnn_hids, cnn_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Convolutional Neural Network with Keras\n",
    "This class creates a convolutional Neural Network with that consists:\n",
    "input layaer, convolutional layers, pooling layers, falltened Neural Network Layers, and output layer\n",
    "'''\n",
    "### Hyper Parameters used to adjust the result ###\n",
    "### More Hyper parameters can be adjusted from 'get_partial' function on line 171 ###\n",
    "epoch = 1000\n",
    "batch_size = 1000\n",
    "### The parameter classes for CNN class ###\n",
    "cnn_input, cnn_convs, cnn_pools, cnn_hids, cnn_output = get_cnn_params()\n",
    "\n",
    "\n",
    "### Data ###\n",
    "X,y = get_data(PATH, kind='train', norm_size=255)\n",
    "np.random.shuffle(X);np.random.shuffle(y)\n",
    "val_X,val_y = get_validation_data(X,y,0.2)      ### use 20% of the training data for validation ###\n",
    "X = np.reshape(X,(X.shape[0],28,28,1))          ### Reshape the flattened image to 3D ###\n",
    "val_X = np.reshape(val_X,(val_X.shape[0],28,28,1))\n",
    "test_X,test_y = get_data(PATH, kind='t10k', norm_size=255)\n",
    "test_X = np.reshape(test_X,(test_X.shape[0],28,28,1))\n",
    "\n",
    "\n",
    "### Convolutional Neural Network ###\n",
    "cnn = CNN(cnn_input, cnn_convs, cnn_pools, cnn_hids, cnn_output)\n",
    "cnn.__build_cnn_model__()\n",
    "cnn.__train__(X,y,(val_X,val_y),epoch,batch_size)\n",
    "\n",
    "plt.plot(np.arange(len(cnn.training_info_.history['acc'])),cnn.training_info_.history['acc'])\n",
    "\n",
    "cnn.__test__(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
